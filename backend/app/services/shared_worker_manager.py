"""Shared worker pool manager for executing trusted functions."""
import asyncio
import io
import json
import tarfile
from datetime import datetime
from typing import Any, Optional

import docker
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import settings

WORKER_EXEC_COUNT_KEY = "sinas:worker:executions"


class SharedWorkerManager:
    """
    Manages a pool of shared worker containers for executing trusted functions.

    Unlike user containers (isolated per-user), workers are shared across all users
    for functions with shared_pool=True.

    Workers can be scaled up/down at runtime via API.
    """

    def __init__(self):
        self.client = docker.from_env()
        self.workers: dict[str, dict[str, Any]] = {}  # worker_id -> worker_info
        self.next_worker_index = 0  # For round-robin load balancing
        self._lock = asyncio.Lock()
        self._initialized = False
        self.docker_network = self._detect_network()

    def _detect_network(self) -> str:
        """Auto-detect Docker network if set to 'auto', otherwise use configured value."""
        network = settings.docker_network

        if network != "auto":
            return network

        # Try to auto-detect by inspecting the backend container
        try:
            import socket

            hostname = socket.gethostname()
            container = self.client.containers.get(hostname)
            networks = list(container.attrs["NetworkSettings"]["Networks"].keys())
            if networks:
                detected = networks[0]
                print(f"üîç Auto-detected Docker network: {detected}")
                return detected
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to auto-detect network: {e}")

        # Fallback to common default
        print("‚ö†Ô∏è  Using fallback network: bridge")
        return "bridge"

    async def initialize(self):
        """
        Initialize worker manager on startup.
        Re-discovers existing worker containers and scales to default count.

        Only called by the backend process (main.py), not by queue workers.
        """
        if self._initialized:
            return

        # Re-discover existing worker containers
        await self._discover_existing_workers()

        # Scale to default count if needed (get db session)
        current_count = len(self.workers)
        if current_count < settings.default_worker_count:
            print(f"üì¶ Scaling to default worker count: {settings.default_worker_count}")
            from app.core.database import AsyncSessionLocal

            async with AsyncSessionLocal() as db:
                await self.scale_workers(settings.default_worker_count, db)

        self._initialized = True
        print(f"‚úÖ Worker manager initialized with {len(self.workers)} workers")

    async def _discover_existing_workers(self):
        """Discover and re-register existing worker containers (including stopped ones)."""
        try:
            # List all containers (including stopped) with sinas-worker-* naming pattern
            containers = self.client.containers.list(all=True, filters={"name": "sinas-worker-"})

            for container in containers:
                container_name = container.name
                # Extract worker number from name (sinas-worker-1 -> 1)
                if container_name.startswith("sinas-worker-"):
                    try:
                        worker_num = container_name.replace("sinas-worker-", "")
                        worker_id = f"worker-{worker_num}"

                        # Start container if it's stopped
                        if container.status != "running":
                            print(f"üîÑ Starting stopped worker: {container_name}")
                            try:
                                container.start()
                                container.reload()  # Refresh status
                            except docker.errors.APIError as start_err:
                                # Container is marked for removal or otherwise unrecoverable
                                print(
                                    f"‚ö†Ô∏è  Cannot start {container_name}, removing: {start_err}"
                                )
                                try:
                                    container.remove(force=True)
                                except Exception:
                                    pass
                                continue

                        # Get container creation time
                        container_info = container.attrs
                        created_at = container_info.get("Created", datetime.utcnow().isoformat())

                        self.workers[worker_id] = {
                            "container_name": container_name,
                            "container_id": container.id,
                            "created_at": created_at,
                            "executions": 0,  # Reset execution count on rediscovery
                        }

                        print(
                            f"üîç Rediscovered worker: {container_name} (status: {container.status})"
                        )
                    except Exception as e:
                        print(f"‚ö†Ô∏è  Failed to process worker {container_name}: {e}")

        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to discover existing workers: {e}")

    def get_worker_count(self) -> int:
        """Get current number of workers."""
        return len(self.workers)

    async def list_workers(self) -> list[dict[str, Any]]:
        """List all workers with status and execution counts from Redis."""
        # Read execution counts from Redis (shared across processes)
        exec_counts: dict[str, int] = {}
        try:
            from redis.asyncio import Redis

            redis = Redis.from_url(settings.redis_url, decode_responses=True)
            raw = await redis.hgetall(WORKER_EXEC_COUNT_KEY)
            exec_counts = {k: int(v) for k, v in raw.items()}
            await redis.aclose()
        except Exception:
            pass

        workers = []
        for worker_id, info in self.workers.items():
            try:
                container = self.client.containers.get(info["container_name"])
                workers.append(
                    {
                        "id": worker_id,
                        "container_name": info["container_name"],
                        "status": container.status,
                        "created_at": info["created_at"],
                        "executions": exec_counts.get(worker_id, 0),
                    }
                )
            except docker.errors.NotFound:
                workers.append(
                    {
                        "id": worker_id,
                        "container_name": info["container_name"],
                        "status": "missing",
                        "created_at": info["created_at"],
                        "executions": exec_counts.get(worker_id, 0),
                    }
                )
        return workers

    async def scale_workers(self, target_count: int, db: AsyncSession) -> dict[str, Any]:
        """
        Scale workers to target count.

        Returns:
            Dict with scaling results
        """
        async with self._lock:
            current_count = len(self.workers)

            if target_count > current_count:
                # Scale up
                added = 0
                for _ in range(target_count - current_count):
                    worker_id = await self._create_worker(db)
                    if worker_id:
                        added += 1

                return {
                    "action": "scale_up",
                    "previous_count": current_count,
                    "current_count": len(self.workers),
                    "added": added,
                }

            elif target_count < current_count:
                # Scale down
                removed = 0
                workers_to_remove = list(self.workers.keys())[target_count:]

                for worker_id in workers_to_remove:
                    if await self._remove_worker(worker_id):
                        removed += 1

                return {
                    "action": "scale_down",
                    "previous_count": current_count,
                    "current_count": len(self.workers),
                    "removed": removed,
                }

            else:
                return {"action": "no_change", "current_count": current_count}

    def _next_worker_number(self) -> int:
        """Find the next available worker number that doesn't collide with existing workers."""
        existing_numbers = set()
        for wid in self.workers:
            # worker IDs are "worker-N"
            try:
                existing_numbers.add(int(wid.split("-", 1)[1]))
            except (IndexError, ValueError):
                pass
        n = 1
        while n in existing_numbers:
            n += 1
        return n

    async def _create_worker(self, db: AsyncSession) -> Optional[str]:
        """Create a new worker container."""
        num = self._next_worker_number()
        worker_id = f"worker-{num}"
        container_name = f"sinas-worker-{num}"

        try:
            # Remove stale container with same name if it exists (e.g. after crash)
            try:
                stale = self.client.containers.get(container_name)
                print(f"üóëÔ∏è  Removing stale container: {container_name}")
                stale.remove(force=True)
            except docker.errors.NotFound:
                pass

            # Create worker container (same security model as user containers)
            container = self.client.containers.run(
                image=settings.function_container_image,  # sinas-executor
                name=container_name,
                detach=True,
                network=self.docker_network,
                mem_limit="1g",
                nano_cpus=1_000_000_000,  # 1 CPU core
                cap_drop=["ALL"],  # Drop all capabilities for security
                cap_add=["CHOWN", "SETUID", "SETGID"],  # Only essential capabilities
                security_opt=["no-new-privileges:true"],  # Prevent privilege escalation
                tmpfs={"/tmp": "size=100m,mode=1777"},  # Temp storage only
                environment={
                    "PYTHONUNBUFFERED": "1",
                    "WORKER_MODE": "true",
                    "WORKER_ID": worker_id,
                },
                # Use default command from image (python3 -u /app/executor.py)
                # Don't override with custom command - executor is needed
                restart_policy={"Name": "unless-stopped"},
            )

            self.workers[worker_id] = {
                "container_name": container_name,
                "container_id": container.id,
                "created_at": datetime.utcnow().isoformat(),
                "executions": 0,
            }

            # Wait for container and executor to be ready
            await asyncio.sleep(2)

            # Install all approved packages in worker
            await self._install_packages(container, db)

            print(f"‚úÖ Created worker: {container_name}")
            return worker_id

        except Exception as e:
            print(f"‚ùå Failed to create worker {container_name}: {e}")
            return None

    async def _install_packages(self, container, db: AsyncSession):
        """
        Install all approved packages in shared worker.

        Shared workers execute any trusted function, so they need all packages.
        """
        from app.models.package import InstalledPackage

        try:
            # Get all approved packages
            result = await db.execute(select(InstalledPackage))
            approved_packages = result.scalars().all()

            if not approved_packages:
                print("üì¶ No approved packages to install in worker")
                return

            # Build package specs with admin-locked versions
            packages_to_install = []
            for pkg in approved_packages:
                if pkg.version:
                    packages_to_install.append(f"{pkg.package_name}=={pkg.version}")
                else:
                    packages_to_install.append(pkg.package_name)

            print(
                f"üì¶ Installing {len(packages_to_install)} packages in worker: {', '.join(packages_to_install)}"
            )

            # Install packages in container
            install_cmd = ["pip", "install", "--no-cache-dir"] + packages_to_install

            exec_result = await asyncio.to_thread(
                container.exec_run,
                cmd=install_cmd,
                demux=True,
            )

            stdout, stderr = exec_result.output
            if exec_result.exit_code == 0:
                print("‚úÖ Successfully installed packages in worker")
            else:
                error_msg = stderr.decode() if stderr else "Unknown error"
                print(f"‚ö†Ô∏è  Package installation had issues in worker: {error_msg}")
                # Don't fail worker creation - log and continue

        except Exception as e:
            print(f"‚ùå Error installing packages in worker: {e}")
            # Don't fail worker creation - log and continue

    async def _remove_worker(self, worker_id: str) -> bool:
        """Remove a worker container."""
        if worker_id not in self.workers:
            return False

        info = self.workers[worker_id]
        container_name = info["container_name"]

        try:
            container = self.client.containers.get(container_name)
            container.stop(timeout=10)
            container.remove()

            del self.workers[worker_id]

            print(f"‚úÖ Removed worker: {container_name}")
            return True

        except docker.errors.NotFound:
            # Already removed
            del self.workers[worker_id]
            return True
        except Exception as e:
            print(f"‚ùå Failed to remove worker {container_name}: {e}")
            return False

    async def reload_packages(self, db: AsyncSession) -> dict[str, Any]:
        """
        Reload packages in all shared workers.
        Reinstalls all approved packages in each worker.
        """
        async with self._lock:
            if not self.workers:
                return {"status": "no_workers", "message": "No workers to reload"}

            success_count = 0
            failed_count = 0
            errors = []

            for worker_id, info in self.workers.items():
                container_name = info["container_name"]
                try:
                    container = self.client.containers.get(container_name)
                    await self._install_packages(container, db)
                    success_count += 1
                    print(f"‚úÖ Reloaded packages in worker: {container_name}")
                except Exception as e:
                    failed_count += 1
                    error_msg = f"Worker {container_name}: {str(e)}"
                    errors.append(error_msg)
                    print(f"‚ùå Failed to reload packages in {container_name}: {e}")

            return {
                "status": "completed",
                "total_workers": len(self.workers),
                "success": success_count,
                "failed": failed_count,
                "errors": errors if errors else None,
            }

    async def execute_function(
        self,
        user_id: str,
        user_email: str,
        access_token: str,
        function_namespace: str,
        function_name: str,
        enabled_namespaces: list[str],
        input_data: dict[str, Any],
        execution_id: str,
        trigger_type: str,
        chat_id: Optional[str],
        db: AsyncSession,
    ) -> dict[str, Any]:
        """
        Execute function in a worker container using round-robin load balancing.
        """
        async with self._lock:
            if not self.workers:
                return {
                    "status": "failed",
                    "error": "No workers available. Please scale workers up first.",
                }

            # Round-robin load balancing
            worker_ids = list(self.workers.keys())
            worker_id = worker_ids[self.next_worker_index % len(worker_ids)]
            self.next_worker_index += 1

            worker_info = self.workers[worker_id]
            container_name = worker_info["container_name"]

        try:
            container = self.client.containers.get(container_name)

            # Fetch function code from database
            from app.models.function import Function

            result = await db.execute(
                select(Function).where(
                    Function.namespace == function_namespace,
                    Function.name == function_name,
                    Function.is_active == True,
                    Function.shared_pool == True,
                )
            )
            function = result.scalar_one_or_none()

            if not function:
                return {
                    "status": "failed",
                    "error": f"Function {function_namespace}/{function_name} not found or not marked as shared_pool",
                }

            # Prepare execution payload with inline code
            payload = {
                "action": "execute_inline",
                "function_code": function.code,
                "execution_id": execution_id,
                "function_namespace": function_namespace,
                "function_name": function_name,
                "enabled_namespaces": enabled_namespaces,
                "input_data": input_data,
                "context": {
                    "user_id": user_id,
                    "user_email": user_email,
                    "access_token": access_token,
                    "execution_id": execution_id,
                    "trigger_type": trigger_type,
                    "chat_id": chat_id,
                },
            }

            # Write payload to container via exec_run + stdin pipe.
            # We cannot use put_archive: it writes to the overlay layer which
            # is invisible through tmpfs mounts on Linux.
            # Stdin piping has no ARG_MAX limit and works with any payload size.
            payload_bytes = json.dumps(payload).encode("utf-8")

            # Step 1: Pipe payload into container via stdin (exec_create + exec_start)
            api = container.client.api
            exec_id = api.exec_create(
                container.id,
                ['python3', '-c', 'import sys; open("/tmp/exec_request.json","wb").write(sys.stdin.buffer.read())'],
                stdin=True,
                stdout=True,
                stderr=True,
            )["Id"]
            sock = api.exec_start(exec_id, socket=True)
            sock._sock.sendall(payload_bytes)
            import socket as _sock_mod
            sock._sock.shutdown(_sock_mod.SHUT_WR)
            sock.read()  # Wait for command to finish
            sock.close()

            # Step 2: Trigger execution and poll for result
            exec_result = await asyncio.to_thread(
                container.exec_run,
                cmd=[
                    "python3",
                    "-c",
                    f"""
import sys, json, time, os
with open("/tmp/exec_trigger", "w") as f:
    f.write("1")
max_wait = {settings.function_timeout}
start = time.time()
while time.time() - start < max_wait:
    try:
        with open("/tmp/exec_result.json", "r") as f:
            result = json.load(f)
            os.remove("/tmp/exec_result.json")
            os.remove("/tmp/exec_trigger")
            print(json.dumps(result))
            sys.exit(0)
    except FileNotFoundError:
        time.sleep(0.1)
        continue
print(json.dumps({{"error": "Execution timeout"}}))
sys.exit(1)
""",
                ],
                demux=True,
            )

            stdout, stderr = exec_result.output
            stdout_str = stdout.decode() if stdout else ""

            if exec_result.exit_code == 0:
                result = json.loads(stdout_str)

                # Track execution count in Redis (shared across processes)
                try:
                    from redis.asyncio import Redis

                    redis = Redis.from_url(settings.redis_url, decode_responses=True)
                    await redis.hincrby(WORKER_EXEC_COUNT_KEY, worker_id, 1)
                    await redis.aclose()
                except Exception:
                    pass  # Non-critical ‚Äî don't fail execution over counter

                return result
            else:
                error_msg = stderr.decode() if stderr else "Unknown error"
                return {"status": "failed", "error": error_msg}

        except Exception as e:
            return {"status": "failed", "error": f"Worker execution failed: {str(e)}"}


# Global worker manager instance
shared_worker_manager = SharedWorkerManager()
